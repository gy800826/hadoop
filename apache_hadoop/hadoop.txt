安裝jdk
rpm -ivh /tmp/jdk-7u79-linux-x64.rpm

建立軟連結
ln -s /usr/java/jdk1.7.0_79/ /usr/java/java

確認連結成功
ls -al /usr/java/

設定環境目錄
vi /etc/profile
export JAVA_HOME=/usr/java/java
export JRE_HOME=$JAVA_HOME/jre
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib/rt.jar
export PATH=$PATH:$JAVA_HOME/bin


檢查版本
java -version

設定安全性
setenforce 0

永久停止selinux安全性
vi /etc/selinux/config
SELINUX=disabled


停止防火牆

service iptables stop
chkconfig iptables off

設定SSH登入免詢問
vi /etc/ssh/ssh_config
StrictHostKeyChecking no

service sshd restart 

啟動ntp
chkconfig ntpd on
service ntpd start

only主機
設定主機資訊
vi /etc/hosts
192.168.214.136 master
192.168.214.137 slaver1
192.168.214.138 slaver2


產生 keygen
ssh-keygen -t rsa -f ~/.ssh/id_rsa -P ""
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
ssh localhost

複製 key 到其他兩台
ssh-copy-id -i ~/.ssh/id_rsa.pub root@slaver1
ssh-copy-id -i ~/.ssh/id_rsa.pub root@slaver2

下載 hadoop 2.2
wget "https://archive.apache.org/dist/hadoop/core/hadoop-2.2.0/hadoop-2.2.0.tar.gz"

安裝hadoop
tar -zxvf /tmp/hadoop-2.2.0.tar.gz
mv hadoop-2.2.0 /opt
新增環境變數
vi /etc/profile
export HADOOP_HOME=/opt/hadoop/
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export YARN_CONF_DIR=$HADOOP_HOME/etc/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib"

source /etc/profile

新增環境變數
vi /opt/hadoop-2.2.0/libexec/hadoop-config.sh
export JAVA_HOME=/usr/java/java


vi /opt/hadoop-2.2.0/etc/hadoop/hadoop-env.sh
export JAVA_HOME=/usr/java/java
export HADOOP_HOME=/opt/hadoop
export PATH=$PATH:$HADOOP_HOME/bin
export PATH=$PATH:$HADOOP_HOME/sbin
export HADOOP_MAPRED_HOME=$HADOOP_HOME
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_HDFS_HOME=$HADOOP_HOME
export YARN_HOME=$HADOOP_HOME
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_HOME/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_HOME/lib"


將mapred-site.xml.template 更名
cp /opt/hadoop-2.2.0/etc/hadoop/mapred-site.xml.template /opt/hadoop-2.2.0/etc/hadoop/mapred-site.xml


vi /opt/hadoop-2.2.0/etc/hadoop/mapred-site.xml
<configuration>
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>
</configuration>

修改 core-site.xml內容
vi /opt/hadoop-2.2.0/etc/hadoop/core-site.xml
<configuration>
<property>
<name>fs.default.name</name>
<value>hdfs://192.168.214.136:9000</value>
</property>
<property>
<name>hadoop.tmp.dir</name>
<value>/opt/hadoop/tmp</value>
</property>
</configuration>


修改 yarn-site.xml內容

vi /opt/hadoop-2.2.0/etc/hadoop/yarn-site.xml
<?xml version="1.0"?>
<configuration>
<property>
<name>yarn.resourcemanager.resource-tracker.address</name>
<value>master:8031</value>
</property>
<property>
<name>yarn.resourcemanager.scheduler.address</name>
<value>master:8030</value>
</property>
<property>
<name>yarn.resourcemanager.scheduler.class</name>
<value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler</value>
</property>
<property>
<name>yarn.resourcemanager.address</name>
<value>master:8032</value>
</property>
<property>
<name>yarn.nodemanager.local-dirs</name>
<value>${hadoop.tmp.dir}/nodemanager/local</value>
</property>
<property>
<name>yarn.nodemanager.address</name>
<value>0.0.0.0:8034</value>
</property>
<property>
<name>yarn.nodemanager.remote-app-log-dir</name>
<value>/logs</value>
</property>
<property>
<name>yarn.nodemanager.log-dirs</name>
<value>${hadoop.tmp.dir}/nodemanager/logs</value>
</property>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>
<property>
<name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
<value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>
<property>
<name>yarn.log-aggregation-enable</name>
<value>true</value>
<description>Configuration to enable or disable log aggregation</description>
</property>
</configuration>


修改 hdfs-site.xml內容
vi /opt/hadoop-2.2.0/etc/hadoop/hdfs-site.xml 



<configuration>
<property>
<name>dfs.replication</name>
<value>2</value>
</property>
<property>
<name>dfs.permissions</name>
<value>false</value>
</property>
</configuration>


複製檔案到其他兩台(在Master做)
scp -rp /etc/hosts root@slaver1:/etc/hosts
scp -rp /opt/hadoop-2.2.0/ root@slaver1:/opt/hadoop-2.2.0
scp -rp /etc/profile root@slaver1:/etc/profile

產生 hadoop 連結檔(在每台電腦做)
ln -s /opt/hadoop-2.2.0 /opt/hadoop
確認連結成功
ls -al /opt/hadoop

關閉 safemode (如果發生無法操作hdfs時)
hadoop dfsadmin -safemode leave
hadoop dfsadmin -refreshNodes

建立tmp目錄
mkdir -p $HADOOP_HOME/tmp
建立slaver資訊
vi /opt/hadoop/etc/hadoop/slaves
slaver1
slaver2

hdfs 格式化
hadoop namenode -format 

萬一format 失敗
rm -r -f $HADOOP_HOME/tmp
再重新建立目錄


curl http://192.168.114.10:9000/getjson  ????


啟用HDFS及YARN
start-dfs.sh
start-yarn.sh
start-all.sh (一次開兩個服務)
stop-all.sh  (一次關兩個服務)

mr-jobhistory-daemon.sh start historyserver


使用 jps 檢視 process

新增datanode

master:
vi /opt/hadoop-2.2.0/etc/hadoop/slaves

newslaver
scp -rp /etc/hosts root@slaver1:/etc/hosts(每台slaver)

vi /etc/hosts
新增
xxx.xxx.xxx.xxx new_slaver

new_slaver:
cd /opt/hadoop-2.2.0/bin
	(hadoop)
hadoop-daemon.sh start datanode
hadoop-daemon.sh start tasktracker

balance

master:

start-balancer.sh -threshold 5

hdfs dfsadmin -setBalancerBandWidth 67108864
同下
在hdfs-site.xml中可以設置數據均衡占用的網絡帶寬限制
默認設置：1048576（1M/S）
vi /opt/hadoop-2.2.0/etc/hadoop/hdfs-site.xml

新增
<property>
<name>dfs.balance.bandwidthPerSec</name>
<value>1048576</value>
</property>
修改dfs.balance.bandwidthPerSech參數後，要在namenode上執行stop-dfs.sh start-dfs.sh重開hdfs才能生效。


刪除節點:
master:
cd /opt/hadoop-2.2.0/etc/hadoop
vi exclude
ip

(datanode)
vi hdfs-site.xml
新增
<property>
<name>dfs.hosts.exclude</name>
<value>/opt/hadoop-2.2.0/etc/hadoop/excludes</value>
</property>

(NodeManager)
vi yarn-site.xml
新增
<property>
<name>yarn.resourcemanager.nodes.exclude-path</name>
<value>/opt/hadoop-2.2.0/etc/hadoop/excludes</value>
</property>



#更新HDFS的配置文件hdfs-site.xml
hdfs dfsadmin -refreshNodes  
#更新YARN的配置文件yarn-site.xml
yarn rmadmin -refreshNodes  


查看運行hdfs dfsadmin -report
	hadoop dfsadmin -report


hadoop fs -put pagecounts-20071209-180000 empty.txt /data

hadoop fs -appendToFile 1.txt 2.txt hdfs:///data1/test.txt

#!/bin/awk -f
BEGIN { FS = "," }
{ print $1","$2","$3$4$5 }


hadoop jar hadoop-mapreduce-examples-2.2.0.jar wordcount hdfs:///data2/sep.txt /output/test2






hadoop jar hadoop-mapreduce-examples-2.2.0.jar wordcount hdfs:///data2/sep.txt /output/test2




